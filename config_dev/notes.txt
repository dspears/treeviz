Some scripts for processing top1m:

dspears@Galileo:~/sites/jet/data$ more *.awk
::::::::::::::
deepDomains.awk
::::::::::::::
awk '{u=$0; gsub("[^\.]",""); print length($0) " " u;}' top-1m.csv | sort -r| more

::::::::::::::
maxDomainLabel.awk
::::::::::::::
awk -F, '{n=split($2,a,"\."); m=0; for (i=1; i<=n; i++) { l=length(a[i]); m=(l>m)?l:m;};print m,$2}' top-1m.csv | sort | more


dspears@Galileo:~/sites/jet/data$ more README.txt
s1 - remove any column titles.
     (used vim)

s2 - sort by number of periods in the url.
     This is so that we identify the names of big sites first, since they will be like google.com.
     Later there could be google.com.tr (or some such), and we'll be able to associate that to
     google because we've already seen "google.com" earlier in the file.

     awk '{u=$0; gsub("[^\.]",""); print length($0) " " u;}' top-1m.s1.csv | sort > top-1m.s2.csv

s3 - add column headings back in.
